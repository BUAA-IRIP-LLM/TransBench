# Test document

[BACK](../README.md)

## environment config

Need to install package `transformers`. Other package has to be installed according to each model. Error log can provide useful information. 


## File Tree

`model/*.py` loads model and inference with modelï¼ŒExcept `UGROUND7B`, all .py provide `run_for_case` function api, load model to GPU once import by other script.
`UGROUND7B` follows the recommended execution approach and is invoked using vLLM.

* `test.py` is the main evaluation script. To facilitate parallel execution across multiple machines, it accepts the arguments `--i` and `--ofN`, where `--i` denotes the current machine's index and `--ofN` specifies the total number of machines. When using this approach, extra script need to be implemented manually to merge all results from each machine.
* `test.py` automatically loads the `run_for_case` function from `model/*.py` and invokes it. The specific model to use is determined by the `--model` argument; acceptable model names are listed within the script.
* The `--mode` argument accepts two options: `test_before_finetune` and `test_after_finetune`, distinguishing whether the model being tested originates from before or after fine-tuning.
* The `--split` argument specifies the dataset split to load, while `--subsplit` selects the specific subset (e.g., test or train) within that split. The `--checkpoint` argument is used to label the output filename. Since distinguishing dataset splits is only necessary after fine-tuning, `--checkpoint` is meaningful only in `test_after_finetune` mode.
* The exact model loading paths are controlled via a dictionary inside `test.py`; users can modify these paths to adjust where models are loaded from, as illustrated below:

```python
model_path_map = {
    "ARIA":"./aria_ui_model",
    "SEECLICK":"./seeclick_model",
    "UGROUND7B":"./UGround_model_7B",
    "AGUVIS":"./aguvis_model",
    "OSATLAS":"./OSAtlas_model",
    "COGAGENT":"./cogagent_model",
    "QWEN25VL" : ""
}
tuned_model_path_map = {
    "ARIA": f"./output/tuned_aria_ui_model_{target_split}_{checkpoint_num}"
}
```

After running the `test` script from the command line within the corresponding Python environment, the script will load the dataset stored in the `./transferability_gui_benchmark` folder located in the current directory. Copy files from [huggingface datasets](https://huggingface.co/datasets/luyuheng/TransBench/tree/main) to this folder first.  
It formats and retrieves the model path, sets it as an environment variable, and then calls the scripts under `model/*.py` to load the model. It iteratively executes the `run_for_case` function to obtain results and attempts to write them to a local result file at `./results/***.json`.

> Since the evaluation supports task distribution across multiple machines, data not processed by the current machine will be represented by `None` as a placeholder. This allows results from multiple machines to be merged later.

## Result Analysis

Once results from all machines are merged into a complete set, the `eval_result` script can be used for analysis. This script requires manually setting hyperparameters at the top, then formats the file paths accordingly and loads the results for evaluation.

## Potential Sources of Variation

1. Model inference uses the officially recommended loading method to achieve peak performance. Consequently, randomness introduced by parameters such as `temperature` may lead to non-deterministic outputs, making it hard to guarantee identical generations across runs.
2. During LoRA parameter initialization in model training, no random seed was specified, which may cause training outcomes to vary due to random initialization.
3. Since the benchmark is generated by an automatic labeling process, it may be upgraded to a new version in the future to improve quality. 




